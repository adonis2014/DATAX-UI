<?xml version="1.0" encoding="UTF-8"?>

<jobs>
	<job id="enhbasereader_to_hdfswriter_job">
		<reader>
			<plugin>enhbasereader</plugin>
			<!-- default: description:hbase table name mandatory:true name:htable -->
			<param key="htable" value="?" />
			<!-- default: description:where the hbase_conf file is mandatory:true name:hbase_conf -->
			<param key="hbase_conf" value="?" />
			<!-- default: description:where the hadoop_conf file is mandatory:true name:hadoop_conf -->
			<param key="hadoop_conf" value="?" />
			<!-- default:UTF-8 range:UTF-8|GBK description:indict the encoding in hbase mandatory:false name:encoding -->
			<param key="encoding" value="UTF-8" />
			<!-- default: range: description:indict which CF:qualifier should be write, split by "," mandatory:false name:columns_key -->
			<param key="columns_key" value="?" />
			<!-- default: range: description:range of rowkey mandatory:false name:rowkey_range -->
			<param key="rowkey_range" value="?" />
			<!-- default:1 range:1-100 description:concurrency of the job mandatory:false name:concurrency -->
			<param key="concurrency" value="1" />
			<!-- default:true range:true,false description:是否分拆合并保存的字段 mandatory:false name:isSaveOneObj -->
			<param key="isSaveOneObj" value="true" />
			<!-- default:data description:合并保存的字段名 mandatory:false name:oneObjColName -->
			<param key="oneObjColName" value="data" />
			<!-- default:EN_User description:合并保存的字段对应对象 EN_User-用户表 mandatory:false name:oneObjName -->
			<param key="oneObjName" value="EN_User" />
			<!-- description:需要分拆保存的字段名集合 mandatory:false name:oneObjColumnNames -->
			<param key="oneObjColumnNames" value="?" />
			<!-- default:cf description:需要合并保存的列族名 mandatory:false name:oneObjFamilyName -->
			<param key="oneObjFamilyName" value="cf" />
		</reader>
		<writer>
			<plugin>hdfswriter</plugin>
			<!-- description:HDFS login account, e.g. username,groupname(groupname...),#password mandatory:false name:ugi -->
			<param key="hadoop.job.ugi" value="?" />
			<!-- description:hadoop-site.xml path mandatory:false name:hadoop_conf -->
			<param key="hadoop_conf" value="?" />
			<!-- description:hdfs dir，hdfs://ip:port/path, or file:///home/taobao mandatory:true name:dir -->
			<param key="dir" value="?" />
			<!-- default:part description:hdfs filename mandatory:false name:prefixname -->
			<param key="prefix_filename" value="part" />
			<!-- default:\t range:\t,\001,",' description:how to seperate fields mandatory:false name:fieldSplit -->
			<param key="field_split" value="\t" />
			<!-- default:\n range:\n description:how to seperate lines mandatory:false name:lineSplit -->
			<param key="line_split" value="\n" />
			<!-- default:UTF-8 range:UTF-8|GBK|GB2312 description:encode mandatory:false name:encoding -->
			<param key="encoding" value="UTF-8" />
			<!-- range: description:how to replace null in hdfs mandatory:false name:nullChar -->
			<param key="nullchar" value="?" />
			<!-- default:org.apache.hadoop.io.compress.DefaultCodec range:org. apache.hadoop.io.compress.BZip2Codec|org.apache.hadoop.io.compress.DefaultCodec|org.apache.hadoop.io.compress.GzipCodec 
				description:compress codecs mandatory:false name:codecClass -->
			<param key="codec_class" value="org.apache.hadoop.io.compress.DefaultCodec" />
			<!-- default:NONE range:BLOCK|NONE|RECORD description:how to compress file mandatory:false name:compressionType -->
			<param key="compression_type" value="NONE" />
			<!-- default:-1 range:[-1-1023] description: mandatory:false name:keyFieldIndex -->
			<param key="key_field_index" value="-1" />
			<!-- default:4096 range:[1024-4194304] description:how much the buffer size is mandatory:false name:bufferSize -->
			<param key="buffer_size" value="4096" />
			<!-- default:TXT range:TXT|SEQ|TXT_COMP|SEQ_COMP description:Filetype TXT->TextFile,SEQ->SequenceFile,TXT_COMP->Compressed TextFile,SEQ_COMP->Compressed SequenceFile mandatory:true 
				name:fileType -->
			<param key="file_type" value="TXT" />
			<!-- default:org.apache.hadoop.io.Text range:org.apache.hadoop.io.Text|org.apache.hadoop.io.LongWritable|org.apache.hadoop.io.IntWritable|org.apache.hadoop.io.DoubleWritable|org.apache.hadoop.io.BooleanWritable|org.apache.hadoop.io.ByteWritable|org.apache.hadoop.io.VIntWritable|org.apache.hadoop.io.VLongWritable|org.apache.hadoop.io.NullWritable 
				description:SequenceFile key class mandatory:false name:keyClass -->
			<param key="key_class" value="org.apache.hadoop.io.Text" />
			<!-- default:org.apache.hadoop.io.Text range:org.apache.hadoop.io.Text|org.apache.hadoop.io.LongWritable|org.apache.hadoop.io.IntWritable|org.apache.hadoop.io.DoubleWritable|org.apache.hadoop.io.BooleanWritable|org.apache.hadoop.io.ByteWritable|org.apache.hadoop.io.VIntWritable|org.apache.hadoop.io.VLongWritable|org.apache.hadoop.io.NullWritable 
				description:SequenceFile value class mandatory:false name:valueClass -->
			<param key="value_class" value="org.apache.hadoop.io.Text" />
			<!-- default:3 range:[0-4] description:do clean data before loading 0: overwrite file with the same filename 1: report error when exists same filename 2: delete single file 3: delete 
				all files with the same prefix name 4: delete all files in the directory mandatory:false name:delMode -->
			<param key="del_mode" value="3" />
			<!-- default:1 range:1-100 description:concurrency of the job mandatory:false name:concurrency -->
			<param key="concurrency" value="1" />
		</writer>
	</job>
</jobs>
